{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3c3176",
   "metadata": {
    "papermill": {
     "duration": 0.002946,
     "end_time": "2025-11-04T07:55:35.633175",
     "exception": false,
     "start_time": "2025-11-04T07:55:35.630229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training\n",
    "\n",
    "### Below you will find the code for training the Char-RNN model. **Make sure to enable GPU acceleration (T4 x2) in kaggle before proceeding with this assignment**\n",
    "\n",
    "\n",
    "You are encouraged to make adjustments to the below hyperparameters as you wish.\n",
    "Note that 50 epochs require about 1 hour of training on Kaggle GPU with the provided params.\n",
    "When trained for 50 epochs these params give Train Loss: 1.0388 and Val Loss: 1.0015. The code automatically saves checkpoints every 10 epochs which can be adjust based on your preference. The code also does a basic level of logging of the loss values which is saved as a csv file\n",
    "\n",
    "**Since training the model might take several hours if you're new to Kaggle we recommend using the Save Version button on the top right along with Save & Run All(Commit). This will run all the cell in this notebook in the background since running the cells in this notebook editor will be automatically stopped after 30 minutes of inactivity. Once you use the save option you can view the output logs and files once all the code blocks finish running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6186f990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T07:55:35.639312Z",
     "iopub.status.busy": "2025-11-04T07:55:35.639098Z",
     "iopub.status.idle": "2025-11-04T11:13:54.286272Z",
     "shell.execute_reply": "2025-11-04T11:13:54.285254Z"
    },
    "papermill": {
     "duration": 11898.652155,
     "end_time": "2025-11-04T11:13:54.287954",
     "exception": false,
     "start_time": "2025-11-04T07:55:35.635799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Total documents in dataset: 45458\n",
      "Documents after filtering: 43168\n",
      "Filtered out 2290 documents\n",
      "Total characters in corpus: 79917160\n",
      "Vocabulary size: 192\n",
      "First 50 characters in vocab: \n",
      " !\"$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQ\n",
      "\n",
      "Training set size: 71925444 characters\n",
      "Validation set size: 7991716 characters\n",
      "\n",
      "Number of training batches: 1405\n",
      "Number of validation batches: 157\n",
      "Using 2 GPUs with DataParallel\n",
      "\n",
      "Model has 4,328,640 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:12<00:00,  7.29it/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.9693\n",
      "Val Loss: 1.5281\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.5281)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe that you should have a ost of when it I cannot pamint under agimations, as will in the nebligation of even in Ramnagab, replied ecerpibed to conseitate at nation to a send to be in the one. I better was a resolution. The represences to says, such not so more any satyaglaty and a nighted saja your condac\n",
      "============================================================\n",
      "\n",
      "Epoch 2/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:27<00:00,  6.77it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.2528\n",
      "Val Loss: 1.1015\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.1015)\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:27<00:00,  6.76it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.1467\n",
      "Val Loss: 1.0686\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0686)\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:28<00:00,  6.75it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.1211\n",
      "Val Loss: 1.0544\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0544)\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:30<00:00,  6.69it/s, loss=1.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.1066\n",
      "Val Loss: 1.0437\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0437)\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:28<00:00,  6.74it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0965\n",
      "Val Loss: 1.0367\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0367)\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:30<00:00,  6.67it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0889\n",
      "Val Loss: 1.0319\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0319)\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:30<00:00,  6.69it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0829\n",
      "Val Loss: 1.0268\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0268)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe thatters today. And has it been such a committerictings to the Colour ited laterics. I dongred the Gandi Shri Vanik Mahommin Sud tod earntended by objectings oftended. The Muslim Confectings it is said to doned to a large goverty. The notematurn permffectings only will the some in it afters the that. Th\n",
      "============================================================\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:30<00:00,  6.68it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0778\n",
      "Val Loss: 1.0230\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0230)\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:31<00:00,  6.64it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0735\n",
      "Val Loss: 1.0193\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0193)\n",
      "\n",
      "Epoch 11/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:30<00:00,  6.67it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0701\n",
      "Val Loss: 1.0172\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0172)\n",
      "\n",
      "Epoch 12/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:31<00:00,  6.65it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0668\n",
      "Val Loss: 1.0143\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0143)\n",
      "\n",
      "Epoch 13/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:32<00:00,  6.62it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0639\n",
      "Val Loss: 1.0122\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0122)\n",
      "\n",
      "Epoch 14/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:32<00:00,  6.62it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0614\n",
      "Val Loss: 1.0116\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0116)\n",
      "\n",
      "Epoch 15/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:32<00:00,  6.60it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0589\n",
      "Val Loss: 1.0091\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0091)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe thatand bected by trendly redully attly to. And I have new to only graced to dond thememe. I hadd to all worry tome behart therrity on and hend to me tor mand the in yours tongurn has betward the in Previdum. I will learty and ably gorty to wherebe ton torteding the torrey ton the Kalthway The Botha, Ma\n",
      "============================================================\n",
      "\n",
      "Epoch 16/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.59it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0567\n",
      "Val Loss: 1.0081\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0081)\n",
      "\n",
      "Epoch 17/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0545\n",
      "Val Loss: 1.0066\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0066)\n",
      "\n",
      "Epoch 18/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0527\n",
      "Val Loss: 1.0042\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0042)\n",
      "\n",
      "Epoch 19/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.59it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0509\n",
      "Val Loss: 1.0039\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0039)\n",
      "\n",
      "Epoch 20/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.59it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0492\n",
      "Val Loss: 1.0021\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0021)\n",
      "\n",
      "Epoch 21/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.54it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0478\n",
      "Val Loss: 1.0011\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 1.0011)\n",
      "\n",
      "Epoch 22/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0463\n",
      "Val Loss: 0.9998\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9998)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe thatty tonst test to the mattend no meass on on tol ongly thes to an bect to hespeast tivest give prouctiff and as tols he tead in ally again ites to dees to forty. I hand wark at and and fourty afte tors. We wand ther weakes forty forty forty. The was a men there tare harge tatked any to hergly in past\n",
      "============================================================\n",
      "\n",
      "Epoch 23/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.55it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0451\n",
      "Val Loss: 0.9982\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9982)\n",
      "\n",
      "Epoch 24/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0438\n",
      "Val Loss: 0.9978\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9978)\n",
      "\n",
      "Epoch 25/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.54it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0427\n",
      "Val Loss: 0.9976\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9976)\n",
      "\n",
      "Epoch 26/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0416\n",
      "Val Loss: 0.9967\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9967)\n",
      "\n",
      "Epoch 27/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0406\n",
      "Val Loss: 0.9953\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9953)\n",
      "\n",
      "Epoch 28/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0396\n",
      "Val Loss: 0.9953\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9953)\n",
      "\n",
      "Epoch 29/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0387\n",
      "Val Loss: 0.9950\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9950)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe that beff evendignst I dark wand leasssinttity tork forty witimattly but as gregry on uppress to me. Q. I have my learty towad forty on ongeridno mit assons but torkett forty to to to thery to thes ally as to nonst and any forty ity to agingly wort toest. There furty mente himbess on to stand gon in hea\n",
      "============================================================\n",
      "\n",
      "Epoch 30/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.55it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0378\n",
      "Val Loss: 0.9939\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9939)\n",
      "\n",
      "Epoch 31/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0369\n",
      "Val Loss: 0.9931\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9931)\n",
      "\n",
      "Epoch 32/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.54it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0362\n",
      "Val Loss: 0.9920\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9920)\n",
      "\n",
      "Epoch 33/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.59it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0354\n",
      "Val Loss: 0.9922\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 34/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0347\n",
      "Val Loss: 0.9917\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9917)\n",
      "\n",
      "Epoch 35/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.54it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0340\n",
      "Val Loss: 0.9909\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9909)\n",
      "\n",
      "Epoch 36/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.55it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0335\n",
      "Val Loss: 0.9905\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9905)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe thaty tos to thard int to bey on beft tingly. Our concects haves tout on an out a 1934 13 and I aght on tindd tes to beft to thes vegottly befect. This will best onced to mittinded ton thes. The Apriss wenters froutind to work and to beff wellds geand to faid it sucty is timpst conttceas. But to cause t\n",
      "============================================================\n",
      "\n",
      "Epoch 37/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:35<00:00,  6.53it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0327\n",
      "Val Loss: 0.9897\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9897)\n",
      "\n",
      "Epoch 38/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0321\n",
      "Val Loss: 0.9887\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9887)\n",
      "\n",
      "Epoch 39/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0316\n",
      "Val Loss: 0.9898\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 40/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:35<00:00,  6.52it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0311\n",
      "Val Loss: 0.9885\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9885)\n",
      "\n",
      "Epoch 41/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.58it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0304\n",
      "Val Loss: 0.9866\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9866)\n",
      "\n",
      "Epoch 42/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0300\n",
      "Val Loss: 0.9879\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 43/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0294\n",
      "Val Loss: 0.9863\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9863)\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe thatdly way an wity as my fast ton wight tot inces ons and durrably witry formitty for tand tombly to beces oncedd to beft missit and as tend to tempt ton ity tom ton ity tity ton timbly as torty on any tist on arady tindity to man. Fort I mempty ton on timbass: “Thermpece oncequst tist alty arity witen\n",
      "============================================================\n",
      "\n",
      "Epoch 44/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0290\n",
      "Val Loss: 0.9861\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9861)\n",
      "\n",
      "Epoch 45/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.56it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0284\n",
      "Val Loss: 0.9861\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 46/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.55it/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0281\n",
      "Val Loss: 0.9856\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9856)\n",
      "\n",
      "Epoch 47/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0275\n",
      "Val Loss: 0.9844\n",
      "Learning Rate: 0.002000\n",
      "✓ Saved best model (val_loss: 0.9844)\n",
      "\n",
      "Epoch 48/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:35<00:00,  6.53it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0271\n",
      "Val Loss: 0.9849\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 49/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:34<00:00,  6.54it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0269\n",
      "Val Loss: 0.9851\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "Epoch 50/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1405/1405 [03:33<00:00,  6.57it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.0264\n",
      "Val Loss: 0.9852\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "============================================================\n",
      "Sample generation:\n",
      "============================================================\n",
      "I believe that on on yolk ond whake ton yectly 's. ist tind thes to hearculty torty on tatmon suconcest tight onsind I flictly onsidgind mist itty to tainty to way. Themmes tombly tort want oncevect torty tors wherond to mattly forty no mist tress to yolks on taity lesst ind timps to. T2- Vand tindisticumps Raisf\n",
      "============================================================\n",
      "\n",
      "Training history saved to /kaggle/working/training_metrics.csv\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n",
      "Best validation loss: 0.9844\n",
      "Models saved in: /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Character-Level LSTM Training Script for Mahatma Gandhi Dataset \n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.amp\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETERS - Recommended Starting Values\n",
    "# ============================================================================\n",
    "\n",
    "# Model architecture\n",
    "HIDDEN_SIZE = 512           # Size of LSTM hidden state (256, 512, or 1024)\n",
    "NUM_LAYERS = 2              # Number of LSTM layers (2-3 recommended)\n",
    "EMBEDDING_DIM = 128         # Character embedding dimension\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 512             # Batch size (256 also works)\n",
    "SEQ_LENGTH = 100            # Sequence length for training (50-200)\n",
    "LEARNING_RATE = 0.002       # Learning rate (0.001-0.003 for Adam)\n",
    "NUM_EPOCHS = 50             # Number of training epochs\n",
    "GRADIENT_CLIP = 5.0         # Gradient clipping threshold\n",
    "\n",
    "# Regularization\n",
    "DROPOUT = 0.3               # Dropout rate (0.2-0.5)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '/kaggle/input/collected-works-mahatma-gandhi-a-json-dataset/The-Collected-Works-Mahatma-Gandhi.json'\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "\n",
    "# Define blacklist for unsuitable document types to exclude\n",
    "BLACKLIST_DOCUMENT_TYPE = ['TELEGRAM', 'CABLE']\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_prepare_data(data_path, blacklist_types):\n",
    "    \"\"\"Load JSON dataset and extract training text from contents field.\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Total documents in dataset: {len(data)}\")\n",
    "    \n",
    "    # Filter out blacklisted document types\n",
    "    filtered_data = [doc for doc in data if doc.get('document_type') not in blacklist_types]\n",
    "    print(f\"Documents after filtering: {len(filtered_data)}\")\n",
    "    print(f\"Filtered out {len(data) - len(filtered_data)} documents\")\n",
    "    \n",
    "    # Extract all text from 'contents' field\n",
    "    all_text = []\n",
    "    for doc in filtered_data:\n",
    "        content = doc.get('contents', '')\n",
    "        if content:\n",
    "            # Remove footnote markers like {1}, {2}, etc.\n",
    "            content = re.sub(r'\\{\\d+\\}', '', content)\n",
    "            all_text.append(content)\n",
    "    \n",
    "    # Join all text with newlines\n",
    "    text = '\\n\\n'.join(all_text)\n",
    "    print(f\"Total characters in corpus: {len(text)}\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_char_mappings(text):\n",
    "    #Create character to index and index to character mappings.\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"First 50 characters in vocab: {''.join(chars[:50])}\")\n",
    "    \n",
    "    return char_to_idx, idx_to_char, vocab_size\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "class CharDataset(Dataset):\n",
    "    #Character-level dataset for sequence modeling.\n",
    "    def __init__(self, text, char_to_idx, seq_length):\n",
    "        self.text = text\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Encode the entire text\n",
    "        self.encoded = [char_to_idx[ch] for ch in text]\n",
    "        \n",
    "        # Calculate number of sequences\n",
    "        self.num_sequences = len(self.encoded) // seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.seq_length\n",
    "        end_idx = start_idx + self.seq_length + 1\n",
    "        \n",
    "        # Get sequence and target (shifted by 1)\n",
    "        sequence = self.encoded[start_idx:end_idx]\n",
    "        \n",
    "        # Input and target\n",
    "        x = torch.tensor(sequence[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(sequence[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL DEFINITION\n",
    "# ============================================================================\n",
    "class CharLSTM(nn.Module):\n",
    "    #Character-level LSTM language model.\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout (applied after LSTM)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.fc(lstm_out)  # (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        # Return only logits to avoid DataParallel gather shape issues on `hidden`\n",
    "        return output\n",
    "    \n",
    "    def forward_hidden(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        #Initialize hidden state.\n",
    "        weight = next(self.parameters())\n",
    "        hidden = (\n",
    "            weight.new_zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "            weight.new_zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        )\n",
    "        return hidden\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level LSTM language model with Additive (Bahdanau) self-attention.\n",
    "    - Embedding -> LSTM -> Additive attention over LSTM outputs with causal mask -> final FC logits\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, attn_dim=None):\n",
    "        super(CharLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout\n",
    "\n",
    "        # Embedding layer (chars -> vectors)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.emb_ln = nn.LayerNorm(embedding_dim)    # normalize token embeddings\n",
    "\n",
    "        # normalize LSTM outputs before attention projections\n",
    "        self.pre_attn_ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # post-attention LayerNorm (after residual combine)\n",
    "        self.post_attn_ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # LSTM layers (batch_first=True -> input/outputs are (B, T, D))\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Attention dimensionality (internal additive attention dim)\n",
    "        # default: half the hidden size (but at least 1)\n",
    "        if attn_dim is None:\n",
    "            self.attn_dim = max(1, hidden_size // 2)\n",
    "        else:\n",
    "            self.attn_dim = attn_dim\n",
    "\n",
    "        # Additive attention parameters.\n",
    "        # We follow Bahdanau-style score: score(q_i, k_j) = v^T tanh(W_q q_i + W_k k_j)\n",
    "        # Here, q_i and k_j are both the LSTM outputs at different positions.\n",
    "        self.W_q = nn.Linear(hidden_size, self.attn_dim, bias=False)  # project query (per-position)\n",
    "        self.W_k = nn.Linear(hidden_size, self.attn_dim, bias=False)  # project key (per-position)\n",
    "        # v maps the tanh(...) -> scalar score\n",
    "        self.v = nn.Linear(self.attn_dim, 1, bias=False)\n",
    "\n",
    "        # Value projection: project LSTM output into \"value\" space (we keep same hidden_size for residuals)\n",
    "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Optional output projection / residual combine (after attention + residual)\n",
    "        self.output_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Final classifier: map from hidden-size per timestep -> vocab logits\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Dropout after LSTM / attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights (standard, keep final layers small)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Kaiming init for linear layers except final small initial for v/fc output\n",
    "        for m in [self.W_q, self.W_k, self.W_v, self.output_proj]:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        # Initialize v (score vector) with small values to avoid large initial scores\n",
    "        nn.init.uniform_(self.v.weight, -1e-3, 1e-3)\n",
    "        if self.v.bias is not None:\n",
    "            nn.init.constant_(self.v.bias, 0.0)\n",
    "\n",
    "        # Final fc tiny init => helps stabilize initial Q/softmax behavior\n",
    "        nn.init.uniform_(self.fc.weight, -1e-3, 1e-3)\n",
    "        if self.fc.bias is not None:\n",
    "            nn.init.constant_(self.fc.bias, 0.0)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass returning logits for each position.\n",
    "        x: (batch_size, seq_len) long tensor of token indices\n",
    "        hidden: optional LSTM hidden (h0, c0)\n",
    "        returns: logits (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)                        # (B, T, E)\n",
    "        embedded = self.emb_ln(embedded)\n",
    "        # 2) LSTM -> outputs for all timesteps\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)     # (B, T, H), hidden=(h_n, c_n)\n",
    "        lstm_out = self.dropout(lstm_out)                  # (B, T, H)\n",
    "        lstm_norm = self.pre_attn_ln(lstm_out)\n",
    "        # 3) Additive (Bahdanau) attention (causal)\n",
    "        # Project to attention space\n",
    "        # Q = W_q(lstm_out)  -> (B, T, A)\n",
    "        # K = W_k(lstm_out)  -> (B, T, A)\n",
    "        Q = self.W_q(lstm_norm)\n",
    "        K = self.W_k(lstm_norm)\n",
    "\n",
    "        # We will compute additive scores for every pair (i query, j key):\n",
    "        # score_{i,j} = v^T tanh(Q_i + K_j)\n",
    "        # Efficiently compute via broadcasting:\n",
    "        # Q.unsqueeze(2): (B, T_q, 1, A)\n",
    "        # K.unsqueeze(1): (B, 1, T_k, A)\n",
    "        # Q_plus_K -> (B, T, T, A)\n",
    "        # v(tanh(...)) -> (B, T, T, 1) -> squeeze -> (B, T, T)\n",
    "        # Note: here T_q == T_k == seq_len (self-attention)\n",
    "\n",
    "        B, T, _ = Q.size()\n",
    "        device = Q.device\n",
    "\n",
    "        # Expand for pairwise additive combination (broadcast sum)\n",
    "        # Use `unsqueeze` rather than `repeat` to keep memory friendly\n",
    "        Q_exp = Q.unsqueeze(2)         # (B, T, 1, A)\n",
    "        K_exp = K.unsqueeze(1)         # (B, 1, T, A)\n",
    "        # additive combination and non-linearity\n",
    "        additive = torch.tanh(Q_exp + K_exp)   # (B, T, T, A)\n",
    "\n",
    "        # linear map v: produce scalar scores for each (i, j)\n",
    "        scores = self.v(additive).squeeze(-1)  # (B, T, T)\n",
    "\n",
    "        # 4) Causal mask: positions j > i are masked out (lower-triangular allowed)\n",
    "        # Build a (T, T) lower-triangular boolean mask: True for allowed entries (j <= i)\n",
    "        # We keep this on the same device\n",
    "        with torch.no_grad():\n",
    "            causal_mask = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))  # (T, T)\n",
    "\n",
    "        # broadcast mask to (B, T, T) and set disallowed positions to a large negative value\n",
    "        # so softmax ~ 0 there.\n",
    "        # determine an appropriate fill scalar for the scores' dtype\n",
    "        fill_value = torch.tensor(torch.finfo(scores.dtype).min, device=scores.device, dtype=scores.dtype)\n",
    "        \n",
    "        # mask out illegal future positions with the dtype-appropriate minimum\n",
    "        scores = scores.masked_fill(~causal_mask.unsqueeze(0), fill_value)\n",
    "\n",
    "        # 5) Softmax over keys (last dimension j), producing attention weights for each query i\n",
    "        attn_weights = torch.softmax(scores, dim=-1)   # (B, T, T)\n",
    "\n",
    "        # 6) Values projection\n",
    "        V = self.W_v(lstm_out)                         # (B, T, H)   -> projected values\n",
    "\n",
    "        # 7) Compute attention output: for each query position i, weighted sum over keys j\n",
    "        # attn_weights (B, T, T) @ V (B, T, H) -> (B, T, H)\n",
    "        attn_out = torch.bmm(attn_weights, V)          # (B, T, H)\n",
    "\n",
    "        # 8) Residual connection: combine attention output with LSTM output\n",
    "        # and optionally apply a small output projection & non-linearity\n",
    "        combined = attn_out + lstm_out                # (B, T, H)  residual\n",
    "        combined = torch.tanh(self.output_proj(combined))  # (B, T, H)\n",
    "\n",
    "        combined = self.dropout(combined)\n",
    "\n",
    "        # 9) Final classifier -> per-timestep logits\n",
    "        logits = self.fc(combined)                     # (B, T, vocab_size)\n",
    "\n",
    "        # Return logits (and keep hidden as internal LSTM hidden if caller provided/needs it)\n",
    "        return logits\n",
    "\n",
    "    def forward_hidden(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Same as forward but returns logits and hidden (useful for generation pipeline).\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)                        # (B, T, E)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)     # (B, T, H)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Attention (same ops as forward)\n",
    "        Q = self.W_q(lstm_out)\n",
    "        K = self.W_k(lstm_out)\n",
    "        B, T, _ = Q.size()\n",
    "        device = Q.device\n",
    "\n",
    "        Q_exp = Q.unsqueeze(2)         # (B, T, 1, A)\n",
    "        K_exp = K.unsqueeze(1)         # (B, 1, T, A)\n",
    "        additive = torch.tanh(Q_exp + K_exp)   # (B, T, T, A)\n",
    "        scores = self.v(additive).squeeze(-1)  # (B, T, T)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            causal_mask = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))\n",
    "        # determine an appropriate fill scalar for the scores' dtype\n",
    "        fill_value = torch.tensor(torch.finfo(scores.dtype).min, device=scores.device, dtype=scores.dtype)\n",
    "        \n",
    "        # mask out illegal future positions with the dtype-appropriate minimum\n",
    "        scores = scores.masked_fill(~causal_mask.unsqueeze(0), fill_value)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        V = self.W_v(lstm_out)                         # (B, T, H)\n",
    "        attn_out = torch.bmm(attn_weights, V)          # (B, T, H)\n",
    "        combined = attn_out + lstm_out\n",
    "        combined = torch.tanh(self.output_proj(combined))\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.fc(combined)                     # (B, T, vocab_size)\n",
    "\n",
    "        return logits, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize LSTM hidden/cell state (num_layers, batch_size, hidden_size)\n",
    "        weight = next(self.parameters())\n",
    "        return (\n",
    "            weight.new_zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "            weight.new_zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS (AMP-enabled)\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, gradient_clip, scaler):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(progress_bar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n",
    "            output = model(x)                 # model(x) now returns only logits\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "        \n",
    "        # scale and backward\n",
    "        scaler.scale(loss).backward()\n",
    "        # unscale before clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    #Evaluate the model.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            output = model(x)   # model(x) returns only logits\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate_text(model, start_str, char_to_idx, idx_to_char, length=500, temperature=0.8):\n",
    "    #Generate text from the model\n",
    "    model.eval()\n",
    "\n",
    "    base_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert start string to indices\n",
    "        chars = [char_to_idx.get(ch, 0) for ch in start_str]\n",
    "        input_seq = torch.tensor([chars], dtype=torch.long).to(device)\n",
    "        \n",
    "        hidden = base_model.init_hidden(1)\n",
    "        hidden = tuple([h.to(device) for h in hidden])\n",
    "        \n",
    "        generated = start_str\n",
    "        \n",
    "        for _ in range(length):\n",
    "            output, hidden = base_model.forward_hidden(input_seq, hidden)\n",
    "            \n",
    "            # Get last output\n",
    "            output = output[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = torch.softmax(output, dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Append to generated text\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            generated += next_char\n",
    "            \n",
    "            # Update input (append last token)\n",
    "            input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    text = load_and_prepare_data(DATA_PATH, BLACKLIST_DOCUMENT_TYPE)\n",
    "    char_to_idx, idx_to_char, vocab_size = create_char_mappings(text)\n",
    "    \n",
    "    # Create datasets\n",
    "    # Use 90% for training, 10% for validation\n",
    "    split_idx = int(len(text) * 0.9)\n",
    "    train_text = text[:split_idx]\n",
    "    val_text = text[split_idx:]\n",
    "    \n",
    "    print(f\"\\nTraining set size: {len(train_text)} characters\")\n",
    "    print(f\"Validation set size: {len(val_text)} characters\")\n",
    "    \n",
    "    train_dataset = CharDataset(train_text, char_to_idx, SEQ_LENGTH)\n",
    "    val_dataset = CharDataset(val_text, char_to_idx, SEQ_LENGTH)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=(device.type == 'cuda'),\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=(device.type == 'cuda'),\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CharLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "    \n",
    "    # Multi-GPU support\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel has {num_params:,} trainable parameters\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, GRADIENT_CLIP, scaler)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Log metrics\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'char_to_idx': char_to_idx,\n",
    "                'idx_to_char': idx_to_char,\n",
    "                'vocab_size': vocab_size,\n",
    "                'hyperparameters': {\n",
    "                    'hidden_size': HIDDEN_SIZE,\n",
    "                    'num_layers': NUM_LAYERS,\n",
    "                    'embedding_dim': EMBEDDING_DIM,\n",
    "                    'seq_length': SEQ_LENGTH,\n",
    "                    'dropout': DROPOUT\n",
    "                }\n",
    "            }, os.path.join(OUTPUT_DIR, 'best_model.pt'))\n",
    "            print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Generate sample text every 5 epochs\n",
    "        if (epoch-1) % 7 == 0:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"Sample generation:\")\n",
    "            print(\"=\"*60)\n",
    "            # generation uses forward_hidden on base model; works whether model is DataParallel or not\n",
    "            sample = generate_text(model, \"I believe that\", char_to_idx, idx_to_char, length=300)\n",
    "            print(sample)\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(OUTPUT_DIR, f'checkpoint_epoch_{epoch}.pt'))\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(os.path.join(OUTPUT_DIR, 'training_metrics.csv'), index=False)\n",
    "    print(f\"\\nTraining history saved to {os.path.join(OUTPUT_DIR, 'training_metrics.csv')}\")\n",
    "    \n",
    "    # Final model save\n",
    "    torch.save({\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'idx_to_char': idx_to_char,\n",
    "        'vocab_size': vocab_size,\n",
    "    }, os.path.join(OUTPUT_DIR, 'final_model.pt'))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved in: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151c60d",
   "metadata": {
    "papermill": {
     "duration": 6.23726,
     "end_time": "2025-11-04T11:14:06.480370",
     "exception": false,
     "start_time": "2025-11-04T11:14:00.243110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "\n",
    "You can run inference on the trained model using the below provided code. Feel free to adjust the starting prompts and parameters as per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc70f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T11:14:18.460029Z",
     "iopub.status.busy": "2025-11-04T11:14:18.459698Z",
     "iopub.status.idle": "2025-11-04T11:14:23.287309Z",
     "shell.execute_reply": "2025-11-04T11:14:23.286523Z"
    },
    "papermill": {
     "duration": 11.004267,
     "end_time": "2025-11-04T11:14:23.288585",
     "exception": false,
     "start_time": "2025-11-04T11:14:12.284318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /kaggle/working/best_model.pt ...\n",
      "Model loaded successfully!\n",
      "Vocabulary size: 192\n",
      "\n",
      "================================================================================\n",
      "GENERATING TEXT SAMPLES\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Prompt: 'I believe that'\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I believe thatlly to thery forty to onstity ontitas tind tindly tort offectindly froums fority tom- solist re ourgand tombest attly fy tombest beity tons. Thew tand mentict ond afft tatker on actind ourse- best te ty omisstabyly wentind us forlikly. 7. A. & C. S. R. SUMAR, I begast froum froughtly ondedd wary ont tons torty to sors form ably tony thevest hes tindly to thest: “We secize ours trectind tonces tort\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Prompt: 'Harijan'\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Harijant on and to Vaid AMRENCELAR COLOPRA ASHRAMATI I haved way ary, tomista : 1. Oncess al ond turity toats on treass tom oncedity and formity beindity perity tond way to evend tingly thirmes to andity way ond ind way herctinty fortistidgly oncendu akly by Ben Brity finced indly for as to to ity anddid to stres to torty offect ond teas, bectindly gracectes oncestly ton anditindd bety ond beces ty on Mr\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Prompt: 'ngr'\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ngrespincects on ordy thest tindly £30. We quirly ongridddly to Vicect faidd £30.0 ave miss torgal. Welly fordwects tor perctes onced tory to secedity ti formed in frusty ton tat on tindgindint traticotity on Indditi ordity onceptid tourts on Themprit handity to to secend my torty own thestes my tomed beity wardly to wardly to fory ond was oniscitably tond thes tindly furts ours onced timisstid trous\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Prompt: 'I want to be remembered as'\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I want to be remembered as meass ton insta evect tindly comet ondity indy insstind tretty to to man tontid ity on a fact right ond to inttcy issseffects tome tors ours monty to to tinked tirish on grinct tingly herty evect ton teppos thest ondity to s” sicectly tons ham timed tons ond was thirks on frity an reat sectity onced to fortiss yectinty tom torty on ourt on tindes ond to. Head Brindi Hind wardly beces try tingored\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Prompt: 'Dear Friend,'\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Dear Friend, froum, teass ton actind ity oncemptind ty to stind hiss to to to knceed Ayes but tindg way frunddaty I mistably tours to- as aid fully tom oncemd to mardalind ind conty befect ondity and istindity thry hercy was forty on Brity on Allik? I relist try teass aty tons on temps the vect, af teves tindgly bect, to teveceas oncemed any stry ons on actinty to Kangh Band tind, I may noomy tom, Shourish EL\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 0.1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I want to be remembered as miss taity to taity to to to tably tons torty to to to to to tably tons on tindity to thes tindly to to tindly tindd menty tom onced to to to to to to to to to to to too to to to to to to tindly tindly to timps ? What oncedly to oncemptind tindd mentity ton tind to to to to to to to to to to to to \n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 0.5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I want to be remembered ass torty to sords to to years on actind tindly to tindly to my ton tindity torty on tindin trecess to to to to durty ours on on tindgind tind tundatinty teppherty on timbly to try torty on tindgerty on tinds and timp teass ond uphed tind forty to to to to to to tort tor on ity on ity intcitatinddittt\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 1.5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I want to be remembered as goon, nork anty weddingres's indus. VAPLITLY QAR Kopaklk on Plect oncides, forityabsh deenc. Weck Ondelsi. Minddd “”( 6tal .IBarsly Law 7loy-heatis. Amphri, Auery Vinttyah’s hend 7tlickllty-Garity Cont I oncidgaff forddnoon amongidge forwardst J's, 8s—HRI. XM. EHAND, If anyot prit ordity vend ourst\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 2.0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "I want to be remembered asescvaty. Deevoceyably nkly inty Bfight Uprain-Brlmeuk Trisk ant.’bol “Lally Indeceslklyma Hinstts-Nqnd.\n",
      "Jakrif waneskind”aboit ockgh. GB; onceaft’s; gruspvecntidss vids for-jeEhtPultnydampedm, u Ieme. Q SRY, 69 D. Jyesongo/2glie ul “at SIJVco-, Hilkms heveitisefyect“Grgrecth defbe “sugjy, teglge tgk\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 0.1\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Dear Friend, as forty on timbly tons ond to to to to yend timp ton tind timp to to to to to to to to to timp tority on tindind to to thest tind tind timp to to to to to to to to to ? What has tons on actind ton timp torty to to to to to on ond tindgety on timbly tons ond timp torty on timbly tons torty to to to\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 0.5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Dear Friend, to my onceit ty on Inddity Consts fordd ton tat tindity to thes tindly to to Sam Mandd and mentty tors to on tind timp ton Indi tind dut torty to to to to to torty to to to tor try onttly to thest tindly to thest at tind to to deceas oncemptind tress to to gind wissocked too timp torty to to to for\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 1.5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Dear Friend,’s , on I, is light I- V’KBR. Yoursed becentidsitt 08R Opat 9, ons. Almend traiss Mahl’s Meddarybitel vesbourdin KeeCandd Duddes’styo froy, my I I’, £1,,004 buaris 326—Mfmri-du fight, Vedam . .IPKAR GARI, Kelpory, Cambly yoeccably fourt andorecitty is Frand-exist K.N.E. and twanty, fix, witndlinin 2\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Temperature: 2.0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Dear Friend, I begi,22 ANBURARGA Whitly purO necity fosh—C.1. to veas o, Villy, If rictly onl tizeonid Asslvankry, jvesp ‘w! ? C2S,& mandg. Fri Muck que ags fquoulplaty 85, bradint gro. As,\n",
      "\n",
      "OFWN. :yDabu olttytef-p AR6EZBOZASHIBUF, C.D attt…Jerndei-nchawiyceiEtes loisJU Mohs, amVarn. deveDry ppI insdito tolmbod\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Inference Script for Character-Level LSTM Model \n",
    "# ===============================================\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "MODEL_PATH = '/kaggle/working/best_model.pt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _strip_module_prefix(state_dict):\n",
    "    new_state = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k\n",
    "        if k.startswith('module.'):\n",
    "            new_key = k[len('module.'):]\n",
    "        new_state[new_key] = v\n",
    "    return new_state\n",
    "\n",
    "def load_model(model_path):\n",
    "    # Load a trained model from checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    hyperparams = checkpoint.get('hyperparameters', {})\n",
    "    vocab_size = checkpoint['vocab_size']\n",
    "    \n",
    "    # Create model instance (same architecture as training)\n",
    "    model = CharLSTM(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=hyperparams.get('embedding_dim', 128),\n",
    "        hidden_size=hyperparams.get('hidden_size', 512),\n",
    "        num_layers=hyperparams.get('num_layers', 2),\n",
    "        dropout=hyperparams.get('dropout', 0.3)\n",
    "    )\n",
    "    \n",
    "    # Prepare state dict\n",
    "    sd = checkpoint['model_state_dict']\n",
    "    sd = _strip_module_prefix(sd)\n",
    "    \n",
    "    model.load_state_dict(sd)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "          pass\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    char_to_idx = checkpoint['char_to_idx']\n",
    "    idx_to_char = checkpoint['idx_to_char']\n",
    "    \n",
    "    return model, char_to_idx, idx_to_char\n",
    "\n",
    "def generate_text_inference(model, start_str, char_to_idx, idx_to_char, length=500, temperature=0.8):\n",
    "    \n",
    "    model.eval()\n",
    "    base_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert start string to indices\n",
    "        chars = [char_to_idx.get(ch, 0) for ch in start_str]\n",
    "        input_seq = torch.tensor([chars], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = base_model.init_hidden(1)\n",
    "        hidden = tuple([h.to(device) for h in hidden])\n",
    "        \n",
    "        generated = start_str\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Use the base model's forward_hidden to obtain (output, hidden)\n",
    "            output, hidden = base_model.forward_hidden(input_seq, hidden)\n",
    "            last_logits = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            next_char_idx = torch.multinomial(probs, 1).item()\n",
    "            next_char = idx_to_char[next_char_idx]\n",
    "            generated += next_char\n",
    "            input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "def main_inference():\n",
    "    model_path = MODEL_PATH\n",
    "    print(f\"Loading model from: {model_path} ...\")\n",
    "    model, char_to_idx, idx_to_char = load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Vocabulary size: {len(char_to_idx)}\")\n",
    "    \n",
    "    prompts = [\n",
    "        \"I believe that\",\n",
    "        \"Harijan\",# non english based memory testing of something extensively present\n",
    "        \"ngr\", # word completion from between (congress, anger)\n",
    "        \"I want to be remembered as\",\n",
    "        \"Dear Friend,\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING TEXT SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        generated = generate_text_inference(model, prompt, char_to_idx, idx_to_char, length=400, temperature=0.8)\n",
    "        print(generated)\n",
    "    \n",
    "    # Example temperature comparisons\n",
    "    prompts = [\"I want to be remembered as\", \"Dear Friend,\"]\n",
    "    temperatures = [0.1, 0.5, 1.5, 2.0]\n",
    "    for prompt in prompts:\n",
    "        for temp in temperatures:\n",
    "            print(f\"\\n{'─'*80}\")\n",
    "            print(f\"Temperature: {temp}\")\n",
    "            print(f\"{'─'*80}\")\n",
    "            generated = generate_text_inference(model, prompt, char_to_idx, idx_to_char, length=300, temperature=temp)\n",
    "            print(generated)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_inference()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8444661,
     "sourceId": 13321260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11939.26035,
   "end_time": "2025-11-04T11:14:31.143522",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-04T07:55:31.883172",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
