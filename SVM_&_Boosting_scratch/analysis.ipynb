{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from re import escape\n",
    "from config import *\n",
    "from svm import *\n",
    "from ensembling import *\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pre_config= PRE_PROCESSING_CONFIG\n",
    "config_name= \"AdaBoost\"\n",
    "required_config_for_preprocessing = pre_config[config_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initially I manipulated threshold variance by hand because i didnt think of changing utils\n",
    "# later I changed utils in order to incorporate that into grid search\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Preprocess the datasets\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 36 trees\n",
      "Score for 36 trees: 0.8752125255048\n",
      "Accuracy for 36 trees: 0.914375\n"
     ]
    }
   ],
   "source": [
    "# Optimal number of trees in AdaBoost\n",
    "\n",
    "train_dataset = train_dict\n",
    "val_dataset = val_dict\n",
    "\n",
    "best_score = 0\n",
    "best_model = None\n",
    "for n_trees in [36]:\n",
    "    model=AdaBoostClassifier(num_trees=n_trees)\n",
    "    model.fit(train_dataset[\"X\"], train_dataset[\"y\"])\n",
    "    val_results = model.predict(val_dataset['X'])\n",
    "    val_score_model, accuracy = val_score(val_dataset['y'], val_results)\n",
    "    print(f\"Score for {n_trees} trees: {val_score_model}\")\n",
    "    print(f\"Accuracy for {n_trees} trees: {accuracy}\")\n",
    "    \n",
    "    \n",
    "# validation f1 score strictly increases with increase in number of trees\n",
    "# variance 0.9 with 40 trees gave validation score of 0.89 and with 10 trees gave 0.856. However n trees took about 12*n seconds.\n",
    "# variance 0.4 with 40 trees gave validation score of 0.857 and with 10 trees gave 0.852. However n trees took about n/10 seconds.\n",
    "# variance 0.7 with 35 trees gave validation score of 0.86 taking 40 seconds.\n",
    "# 0.8 with 40 trees gives 0.8737 but takes 140 seconds\n",
    "# 0.8 with 35 gives 0.8739 but takes 1m 54 secs\n",
    "# 0.8 with 37 gives 0.8736 but takes 120 sec\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 20 trees\n",
      "7 20 0.1 True 0.8540294838976275\n",
      "Fitting with 20 trees\n",
      "7 20 0.2 True 0.8575754782872846\n",
      "Fitting with 20 trees\n",
      "Fitting with 20 trees\n",
      "Fitting with 20 trees\n",
      "Fitting with 20 trees\n"
     ]
    }
   ],
   "source": [
    "# random forest gridsearch.  \n",
    "# all the parameters were not feasible at the same time hence I first optimized forest parameters and later tree parameters (assuming they have less correlation)\n",
    "# HAD TO MODIFY UTILS IN ORDER TO RUN THESE AND FURTHER CELLS\n",
    "best_model=[]\n",
    "best_score=0\n",
    "\n",
    "for thr in [0.35]:\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing, threshold=thr)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing, threshold=thr)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    \n",
    "    for n_trees in [20]:\n",
    "        for bootstrap_fraction in [0.3]:\n",
    "            for weighted_forest in [True]:\n",
    "                for min_samples_split in [7]:\n",
    "                    for max_depth in [20]:\n",
    "                        for min_gain in [0.2]:\n",
    "                            model=RandomForestClassifier(num_trees=n_trees, bootstrap_fraction=bootstrap_fraction,min_samples_split=min_samples_split,max_depth=max_depth,min_gain=min_gain, weighted_forest=weighted_forest)\n",
    "                            model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "                            val_results = model.predict(val_dict['X'])\n",
    "                            val_score_model = val_score(val_dict['y'], val_results)\n",
    "                            if val_score_model > best_score:\n",
    "                                best_score = val_score_model\n",
    "                                best_model = [min_samples_split,max_depth,min_gain,weighted_forest,best_score]\n",
    "                                print(best_model[0],best_model[1],best_model[2],best_model[3],best_model[4])\n",
    " # 0.4 5 0.3 True 0.8595407667322156\n",
    " # 0.35 20 0.4 True 0.8594469888605343\n",
    " # 0.5 3 0.1 True 0.8593139894651725\n",
    " # 0.3 3 0.2 True 0.8583072642147459\n",
    " # 0.3 3 0.3 True 0.8578628030262498\n",
    " # 0.3 20 0.1 True 0.8575444219737729\n",
    " # 0.6 10 0.1 True 0.8561717370019466\n",
    " # 0.3 10 0.5 True 0.854487667351149\n",
    " # 0.3 5 0.5 True 0.8543867058638921\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=[]\n",
    "best_score=0\n",
    "\n",
    "for thr in [0.2,0.4,0.6,0.8]:\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing, threshold=thr)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing, threshold=thr)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    model=SoftMarginSVMQP(C=1e9, kernel='linear')\n",
    "    model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "    val_results = model.predict(val_dict['X'])\n",
    "    val_score_model = val_score(val_dict['y'], val_results)\n",
    "    if val_score_model > best_score:\n",
    "        best_score = val_score_model\n",
    "        best_model = [thr,best_score]\n",
    "        print(best_model[0],best_model[1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=[]\n",
    "best_score=0\n",
    "\n",
    "for thr in [0.2,0.4,0.6,0.8]:\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing, threshold=thr)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing, threshold=thr)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    for gama in [1e-2,1e-1,1,1e1]:\n",
    "    \n",
    "        model=SoftMarginSVMQP(C=1e9, kernel='rbf',gamma=gama)\n",
    "        model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "        val_results = model.predict(val_dict['X'])\n",
    "        val_score_model = val_score(val_dict['y'], val_results)\n",
    "        if val_score_model > best_score:\n",
    "            best_score = val_score_model\n",
    "            best_model = [thr,best_score,gama]\n",
    "            print(best_model[0],best_model[1],best_model[2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=[]\n",
    "best_score=0\n",
    "\n",
    "for thr in [0.2,0.4,0.6,0.8]:\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing, threshold=thr)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing, threshold=thr)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    for c in [1e-2,1e-1,1,1e1,1e2]:\n",
    "    \n",
    "        model=SoftMarginSVMQP(C=c, kernel='linear')\n",
    "        model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "        val_results = model.predict(val_dict['X'])\n",
    "        val_score_model = val_score(val_dict['y'], val_results)\n",
    "        if val_score_model > best_score:\n",
    "            best_score = val_score_model\n",
    "            best_model = [thr,best_score,c]\n",
    "            print(best_model[0],best_model[1],best_model[c])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=[]\n",
    "best_score=0\n",
    "\n",
    "for thr in [0.2,0.4,0.6,0.8]:\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing, threshold=thr)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing, threshold=thr)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    for c in [1e-2,1e-1,1,1e1,1e2]:\n",
    "        for gama in [1e-2,1e-1,1,1e1]:\n",
    "    \n",
    "            model=SoftMarginSVMQP(C=c, kernel='linear')\n",
    "            model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "            val_results = model.predict(val_dict['X'])\n",
    "            val_score_model = val_score(val_dict['y'], val_results)\n",
    "            if val_score_model > best_score:\n",
    "                best_score = val_score_model\n",
    "                best_model = [thr,best_score,c,gama]\n",
    "                print(best_model[0],best_model[1],best_model[2],best_model[3])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 36 trees\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Preprocess the datasets\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    model=AdaBoostClassifier(num_trees=36)\n",
    "    model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "    val_results = model.predict(val_dict['X'])\n",
    "    wrong=[]\n",
    "    for i in range(len(val_results)):\n",
    "        if val_results[i]!=train_dict[\"y\"][i]:\n",
    "             wrong.append(i)\n",
    "        if(len(wrong)==4):\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1051, 1200, 1201, 1202]\n"
     ]
    }
   ],
   "source": [
    "print(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 20 trees\n",
      "[1200, 1201, 1202, 1203]\n"
     ]
    }
   ],
   "source": [
    "pre_config= PRE_PROCESSING_CONFIG\n",
    "config_name= \"RandomForest\"\n",
    "required_config_for_preprocessing = pre_config[config_name]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Preprocess the datasets\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    model=RandomForestClassifier(num_trees=20,bootstrap_fraction=0.3,min_samples_split=7,max_depth=20,min_gain=0.2, weighted_forest=True)\n",
    "    model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "    val_results = model.predict(val_dict['X'])\n",
    "    val_score_model,accuracy = val_score(val_dict['y'], val_results)\n",
    "    \n",
    "    wrong=[]\n",
    "    for i in range(len(val_results)):\n",
    "        if val_results[i]!=train_dict[\"y\"][i]:\n",
    "             wrong.append(i)\n",
    "        if(len(wrong)==4):\n",
    "            break\n",
    "    print(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_config= PRE_PROCESSING_CONFIG\n",
    "config_name= \"hard_margin_linear\"\n",
    "required_config_for_preprocessing = pre_config[config_name]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Preprocess the datasets\n",
    "    train_processor = MNISTPreprocessor('./dataset/train', required_config_for_preprocessing)\n",
    "    train_X, train_y = train_processor.get_all_data()\n",
    "    train_X, train_y = filter_dataset(train_X, train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_y = convert_labels_to_svm_labels(train_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    train_dict = {\"X\": train_X, \"y\": train_y}\n",
    "\n",
    "    val_processor = MNISTPreprocessor('./dataset/val', required_config_for_preprocessing)\n",
    "    val_X, val_y = val_processor.get_all_data()\n",
    "    val_X, val_y = filter_dataset(val_X, val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_y = convert_labels_to_svm_labels(val_y, ENTRY_NUMBER_LAST_DIGIT)\n",
    "    val_dict = {\"X\": val_X, \"y\": val_y}\n",
    "    \n",
    "    model=RandomForestClassifier(num_trees=20,bootstrap_fraction=0.3,min_samples_split=7,max_depth=20,min_gain=0.2, weighted_forest=True)\n",
    "    model.fit(train_dict[\"X\"], train_dict[\"y\"])\n",
    "    val_results = model.predict(val_dict['X'])\n",
    "    wrong=[]\n",
    "    for i in range(len(val_results)):\n",
    "        if val_results[i]!=train_dict[\"y\"][i]:\n",
    "             wrong.append(i)\n",
    "        if(len(wrong)==4):\n",
    "            break\n",
    "    print(wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
